# Training Configuration for CheXpert Project
# Copy this file to .env and modify as needed
# Usage: cp .env.example .env

# Model Architecture (TIMM models)
# EfficientNet V2 (Ross Wightman - recommended): 
#   efficientnetv2_rw_s, efficientnetv2_rw_m, efficientnetv2_rw_l
# EfficientNet V2 (Original Google): 
#   tf_efficientnetv2_s, tf_efficientnetv2_m, tf_efficientnetv2_l
# ResNet: resnet50, resnet101, resnet152
# ResNeXt: resnext50_32x4d, resnext101_32x8d, resnext101_64x4d
# Vision Transformer: vit_base_patch16_224, vit_base_patch32_224, vit_large_patch16_224
# DeiT: deit_base_patch16_224, deit_small_patch16_224, deit_tiny_patch16_224
#       deit_base_distilled_patch16_224, deit_small_distilled_patch16_224
# ConvNeXt: convnext_tiny, convnext_small, convnext_base, convnext_large
MODEL=efficientnetv2_rw_s

# Training Parameters (Fixed for fair comparison)
EPOCHS=50
BATCH_SIZE=32
LEARNING_RATE=0.0001
WEIGHT_DECAY=0.0001
N_SPLITS=5

# Learning Rate Scheduler
# Options: cosine, plateau, none
SCHEDULER=cosine

# Early Stopping
EARLY_STOPPING_PATIENCE=10

# Device Configuration
# Options: cuda, cpu
DEVICE=cuda

# Data Configuration
DATA_DIR=./data/CheXpert-v1.0-small

# Checkpoint Configuration
CHECKPOINT_INTERVAL=10
SAVE_DIR=./checkpoints

# Reproducibility
SEED=42

# Performance settings
NUM_WORKERS=8  # Number of DataLoader workers (set to number of CPUs per GPU)

# HuggingFace config
HUGGINGFACE_HUB_TOKEN=your_token

# ============================================
# Recommended configurations by architecture:
# ============================================

# Small EfficientNet V2 (baseline)
# MODEL=efficientnetv2_rw_s
# BATCH_SIZE=32

# Medium EfficientNet V2 (more capacity)
# MODEL=efficientnetv2_rw_m
# BATCH_SIZE=16

# ResNet50 (classic CNN baseline)
# MODEL=resnet50
# BATCH_SIZE=32

# ResNeXt50 (improved ResNet)
# MODEL=resnext50_32x4d
# BATCH_SIZE=32

# Vision Transformer Base
# MODEL=vit_base_patch16_224
# BATCH_SIZE=16

# DeiT Base (Data-efficient ViT)
# MODEL=deit_base_patch16_224
# BATCH_SIZE=16

# DeiT Small Distilled (pre-distilled from larger model)
# MODEL=deit_small_distilled_patch16_224
# BATCH_SIZE=32

# ConvNeXt Tiny (modern CNN architecture)
# MODEL=convnext_tiny
# BATCH_SIZE=32

# ConvNeXt Base (larger capacity)
# MODEL=convnext_base
# BATCH_SIZE=16

# ============================================
# For CPU training (testing/debugging only)
# ============================================
# DEVICE=cpu
# BATCH_SIZE=8
# EPOCHS=5
# N_SPLITS=2

# ============================================
# For quick testing/debugging
# ============================================
# EPOCHS=5
# N_SPLITS=2
# CHECKPOINT_INTERVAL=2
# EARLY_STOPPING_PATIENCE=3
